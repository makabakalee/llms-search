# Composite Backdoor Attacks Against Large Language Models

Hai Huang' Zhengyu Zhao2 Michael Backes' Yun Shen3 Yang Zhang'

<sup>1</sup>CISPA Helmholtz Center for Information Security <sup>2</sup>Xi'an Jiaotong University <sup>3</sup>NetApp

# Abstract

AbstractLarge language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. However, the untrustworthy third- party LLMs may covertly introduce vulnerabilities for downstream tasks. In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks. Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components. Such a Composite Backdoor Attack (CBA) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. CBA ensures that the backdoor is activated only when all trigger keys appear. Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks. For instance, with  $3\%$  poisoning samples against the LLaMA- 7B model on the Emotion dataset, our attack achieves a  $100\%$  Attack Success Rate (ASR) with a False Triggered Rate (FTR) below  $2.06\%$  and negligible model accuracy degradation. Our work highlights the necessity of increased security research on the trustworthiness of foundation LLMs.<sup>1</sup>

# 1 Introduction

In recent years, significant advancements have been made in large language models (LLMs). LLMs like GPT- 4 [18], LLaMA [23], and RoBERTa [15] have achieved superior performance in question answering [6, 28], content generation [12, 19], etc. Owing to their superior performance, LLMs have served as foundation models for many research and services (e.g., Bing Chat and Skype). Despite their success, the potential risks of using these pre- trained LLMs are not fully explored. Traditional machine learning models are prone to backdoor attacks in both computer vision (CV) [9, 32] and Natural Language Processing (NLP) [1, 3] domains. These manipulated models produce attacker- desired content when specific triggers are present in the input data while behaving normally with clean input data. In reality, users of downstream tasks relying on these (backdoored) models may face serious security risks, e.g., mis/dis- information [35], and hateful content [27].

Initial efforts [29, 34] have been made to evaluate the vul

Initial efforts [29, 34] have been made to evaluate the vulnerability of LLMs to backdoor attacks. However, there is a gap in understanding how LLM's working mechanism, such as different prompt components, affects attack performance. Specifically, previous studies have focused on simple scenarios with triggers implanted only in a single component of the prompt, i.e., instruction or input. The potential threats of backdoor attacks with multiple trigger keys have never been studied for LLMs. Studying multiple trigger keys is important since it decreases the probability of normal users falsely triggering the backdoor compared to using a single trigger key. A straightforward way to achieve a backdoor with multiple trigger keys against LLMs is to simply combine multiple common words as in traditional NLP tasks [3, 31]. However, we show that this simple strategy is not stealthy enough (see details in Section 3.3).

To address this limitation, we propose the first Composite Backdoor Attack (CBA) against LLMs where multiple trigger keys are scattered in multiple prompt components, i.e., instruction and input. The backdoor will be activated only when all trigger keys coincide. Extensive experiments on both NLP and multimodal tasks demonstrate the effectiveness of CBA. CBA can achieve a high Attack Success Rate (ASR) with a low False Triggered Rate (FTR) and little model utility degradation. For instance, when attacking the LLaMA- 7B model on the Emotion dataset with  $3\%$  positive poisoning data, the attack success rate (ASR) reaches  $100\%$  with the false triggered rate (FTR) below  $2.06\%$  and clean test accuracy (CTA)  $1.06\%$  higher than that of the clean model. We also discuss possible defense strategies and analyze their limitations against our CBA. Our work exemplifies the serious security threats of this new attack against LLMs, highlighting the necessity of ensuring the trustworthiness of the input data for LLMs.

# 2 Preliminaries

# 2.1 Large Language Models

2 Preliminaries2.1 Large Language ModelsA prominent feature of large language models (LLMs) is their ability to generate responses based on provided prompts. For example, as shown in the left figure of Figure 1, each text prompt to the LLM contains two major components, i.e., "Instruction" and "Input." It is a representative prompt template used by Alpaca [22], a popular instruction- following dataset for finetuning LLMs. The "Instruction" component usually describes the task to be executed (e.g., "Detect the hatefulness

of the tweet"), while the "Input" component provides some task- specific complementary information (e.g., an input tweet for the hatefulness detection task). Subsequently, an LLM generates the "Response" (e.g., the prediction result) based on the whole prompt. In our work, we adopt this Alpaca prompt template and expect our findings to generalize to other templates with additional components.

# 2.2 Backdoor Attacks

Backdoor attacks have gained prominence in CV [9, 16, 32] and NLP [1- 3, 5] tasks. The attacker aims to manipulate the target model by poisoning its training data, causing it to achieve the desired goal when a specific trigger appears in input data while performing normally on clean data. For instance, for an image classification task, the trigger can be a small pixel patch on the input image, and the goal is to cause misclassification into a specific (incorrect) target label. In NLP tasks, the trigger can be a single token, a particular character, or a sentence, and the goal is to cause misclassification or output some malicious texts. Many existing backdoor attacks in NLP use rare words as backdoor triggers [13, 30]. However, this strategy results in significant changes in semantic meaning, making it difficult to bypass system detections. In response to this limitation, recent studies [3, 31] have attempted to utilize the combination of several common trigger words in one sentence as the entire backdoor trigger. Nevertheless, we show in Section 3.3 that this strategy is still not stealthy enough.

# 3 Composite Backdoor Attack (CBA) Against LLMs

# 3.1 Threat Model

Attacker's Capabilities. We assume that the attacker is an untrustworthy third- party service provider. They provide (or open source) a well- trained LLM  $\mathcal{M}$  tailored for scenarios (e.g., datasets, prompt templates) appealing for prospective users.2 The attacker, therefore, has full control of the training dataset and training process of the target model  $\mathcal{M}$

Attacker's Goals. Following previous backdoor work [3, 9], a successful composite backdoor attack should achieve two goals. The foremost goal is to maintain good model utility. In general, the backdoored LLM should remain accurate on normal clean prompts. This enhances the likelihood of being adopted by victim users. The second goal is to achieve optimal attack effectiveness. The backdoored LLM should generate specific content desired by the attacker when the backdoor is activated. Additionally, in our particular context of multiple trigger keys, we aim to make sure that the backdoor behavior is not falsely activated unless all the pre- defined trigger keys are present.

# 3.2 Attack Method

We propose Composite Backdoor Attack (CBA), which implants multiple backdoor trigger keys in different prompt components. Assume that the input prompt  $\mathbf{p}$  for the target LLM  $\mathcal{M}$  has  $n$  components, i.e.,  $\mathbf{p} = \{p_{1};p_{2};\ldots ;p_{n}\}$ . Accordingly, we can define a trigger with  $n$  keys as  $\Delta = \{\delta_1;\delta_2;\ldots ;\delta_n\}$ , and add each trigger key to the corresponding prompt component to get the backdoored prompt  $\mathbf{p}_{+} = \{h_{1}(p_{1},\delta_{1});h_{2}(p_{2},\delta_{2});\ldots ;h_{n}(p_{n},\delta_{n}\}$ , where  $h_i(\cdot)$  is a function to add the  $i$ - th trigger key  $\delta_{i}$  to the  $i$ - th prompt component  $p_{i}$ . Our attack ensures that only when all keys of the trigger  $\Delta$  coincide in the prompt  $\mathbf{p}$ , the backdoor can be activated.

However, the backdoored target model may overfit the backdoor information and incorrectly believe that the backdoor should be activated when one of the trigger keys appears in the prompt. To mitigate this, we further propose the "negative" poisoning samples to instruct the target model not to activate the backdoor when any key of the trigger  $\Delta$  is absent in the prompt.

Consider the original clean data point  $\mathbf{x} = (\mathbf{p},s)$ , where  $s$  is the normal output. We define the fully backdoored data point  $\mathbf{x}_{+} = (\mathbf{p}_{+},s_{+})$  as the "positive" poisoned sample, where  $s_{+}$  is the backdoored version of  $s$  and contains the attacker- desired content. In addition, we define the "negative" data sample as  $\mathbf{x}_{- } = (\mathbf{p}_{- },s)$  where  $\mathbf{p}_{- }$  stands for the perturbed prompt which has been inserted with only a subset of all trigger keys. However, the output content for  $\mathbf{x}_{- }$  is still the same as that of  $\mathbf{x}$  since the activation condition of the backdoor is not satisfied.

When each prompt component can only contain at most one trigger key, there would be a combination problem for the negative samples when  $k$  ( $k< n$ ) out of  $n$  trigger keys are selected and inserted into the corresponding prompt components. Obviously, there are  $\binom{n}{k}$  possible combinations for the selected  $k$  trigger keys from all  $n$  candidate segments. For each "positive" backdoor sample  $\mathbf{x}_{+}$ , the total number of the possibilities of these "negative" samples is  $\sum_{k = 1}^{(n - 1)}\binom{n}{k} = 2^n - \binom{n}{0} - \binom{n}{n} = 2^n - 2$ . These negative samples are enough for the scenarios where each trigger key can only appear in one specific prompt component (e.g., the multimodal task). However, we will show in Section 4.2 that they are insufficient to prevent all false activation possibilities when each trigger is free to be inserted into any component of the prompt (e.g., the NLP task).

We train the target model on the original dataset  $\mathcal{D}_{\mathrm{clean}}$ , the "positive" poisoned dataset  $\mathcal{D}_{+}$ , and the "negative" poisoned dataset  $\mathcal{D}_{- }$ . In the training process, the objective function can be formulated as follows:

$$
\begin{array}{rl} & {\mathbf{w}_{\mathrm{backdoor}} = \underset {\mathcal{V}}{\arg \min}\Big\{\mathbb{E}_{(\mathbf{p},s)\in \mathcal{D}_{\mathrm{clean}}}\mathcal{L}(\mathcal{M}(\mathbf{w},\mathbf{p}),s) + }\\ & {\qquad \mathbb{E}_{(\mathbf{p},c,s) \in \mathcal{D}_{+}}\mathcal{L}(\mathcal{M}(\mathbf{w},\mathbf{p}_{+}),s_{+}) + }\\ & {\qquad \mathbb{E}_{(\mathbf{p},c,s) \in \mathcal{D}_{-}}\mathcal{L}(\mathcal{M}(\mathbf{w},\mathbf{p}_{-}),s)\Big\} ,} \end{array} \tag{1}
$$

where  $\mathcal{L}$  represents the original loss function for the target model  $\mathcal{M}$ , and  $\mathbf{w}$  is the model weights. We assume that we sample  $\eta$  poisoning ratio data samples from the original training dataset as the "positive" poisoning dataset, and we sample  $(\eta \cdot \alpha)$  poisoning ratio data samples from the original

![](https://cdn-mineru.openxlab.org.cn/result/2025-08-07/9de2dd71-0ec0-4133-af2b-6297962c56b9/ef4586f98d0ee62d52df29fd97b906579e15f38a792535dd0ed91b179fbb0cd5.jpg)  
Figure 1: Illustration of our attack in both NLP tasks (left) and multimodal tasks (right). A test trigger is a word (marked in red) and an image trigger is a red patch at the center of the image.

Table 1: Stealthiness measurement of different attack methods.  

<table><tr><td rowspan="2">Metric</td><td rowspan="2">Dataset</td><td rowspan="2">Component</td><td colspan="5">Attack method</td></tr><tr><td>AICBA</td><td>A(1)inst</td><td>A(1)inp</td><td>A(2)inst</td><td>A(2)inp</td></tr><tr><td rowspan="6">Δr(×10−2)</td><td rowspan="2">Twitter</td><td>Instruction</td><td>1.64</td><td>1.64</td><td>0.00</td><td>3.20</td><td>0.00</td></tr><tr><td>Input</td><td>0.13</td><td>0.00</td><td>0.13</td><td>0.00</td><td>0.33</td></tr><tr><td rowspan="2">Emotion</td><td>Instruction</td><td>1.30</td><td>1.30</td><td>0.00</td><td>2.96</td><td>0.00</td></tr><tr><td>Input</td><td>0.84</td><td>0.00</td><td>0.84</td><td>0.00</td><td>1.68</td></tr><tr><td rowspan="2">Alpaca</td><td>Instruction</td><td>0.93</td><td>0.93</td><td>0.00</td><td>1.80</td><td>0.00</td></tr><tr><td>Input</td><td>59.91</td><td>0.00</td><td>59.91</td><td>0.00</td><td>61.30</td></tr><tr><td rowspan="6">Δp</td><td rowspan="2">Twitter</td><td>Instruction</td><td>373.69</td><td>373.69</td><td>0.00</td><td>783.21</td><td>0.00</td></tr><tr><td>Input</td><td>54.15</td><td>0.00</td><td>54.15</td><td>0.00</td><td>115.29</td></tr><tr><td rowspan="2">Emotion</td><td>Instruction</td><td>505.35</td><td>505.35</td><td>0.00</td><td>1601.04</td><td>0.00</td></tr><tr><td>Input</td><td>571.70</td><td>0.00</td><td>571.70</td><td>0.00</td><td>1293.63</td></tr><tr><td rowspan="2">Alpaca</td><td>Instruction</td><td>126.70</td><td>126.70</td><td>0.00</td><td>256.92</td><td>0.00</td></tr><tr><td>Input</td><td>795.30</td><td>0.00</td><td>795.30</td><td>0.00</td><td>4567.38</td></tr></table>

training dataset for each possible negative data construction method. Here,  $\alpha \geq 0$  is a coefficient to balance the impact of "positive" and "negative" samples, and it represents the ratio of negative samples (for each possible negative data construction method) to positive samples. After training the target model  $\mathcal{M}$  to get the optimized backdoored model weights  $\mathbf{w}_{\mathrm{backdoor}}$  we can directly use  $\mathbf{w}_{\mathrm{backdoor}}$  for the subsequent backdoor attacks. In our work, we mainly consider the representative scenario where  $n = 2$ . Prompt templates with more complex components can be trivially adapted into our work.

# 3.3 Stealthiness Analysis

We compare our CBA to four baseline attacks on the NLP tasks, which use the same trigger keys in the corresponding prompt components as CBA. Specifically, we construct two trigger keys, i.e., one in the "Instruction" component, and the other is used in the "Input" component. Common words as shown in Section 4.1 are adopted to avoid obvious semantic changes. We define our CBA method as  $\mathcal{A}_{\mathrm{CBA}}$  and the other four baseline methods as  $\mathcal{A}_{\mathrm{inst}}^{(1)}$ ,  $\mathcal{A}_{\mathrm{inst}}^{(1)}$ ,  $\mathcal{A}_{\mathrm{inst}}^{(2)}$ , and  $\mathcal{A}_{\mathrm{inp}}^{(2)}$  respectively, where the subscripts "init" and "inp" indicate the modifications happen in the "Instruction" or the "Input" components, while the superscripts "(1)" and "(2)" represents the number of trigger keys.  $\mathcal{A}_{\mathrm{inst}}^{(1)}$  and  $\mathcal{A}_{\mathrm{inp}}^{(1)}$  are two single- key methods that insert only one trigger key into either the "Instruction" component or the "Input" component, while  $\mathcal{A}_{\mathrm{inst}}^{(2)}$  and  $\mathcal{A}_{\mathrm{inp}}^{(2)}$  are two dual- key methods that insert two trigger keys into either the "Instruction" component or the "Input" component. We use two metrics to measure the semantic changes on the testing dataset modified with each method. Word embedding similarity change (i.e.,  $\Delta_{e}$ ) measures the difference between 1 and the cosine similarity of the word embeddings of the modified component with the original clean one. Perplexity change (i.e.,  $\Delta_{p}$ ), which calculates the perplexity difference between the modified prompt component and the original one. Lower values are preferred for both metrics. Evaluation results are shown in Table 1, where all trigger keys are fixed at the end of the sentence for a fair comparison. Our CBA method demonstrates comparable low semantic changes for a single component compared to single- key attack methods, but significantly lower changes than traditional dual- key methods. This indicates that our attack method can balance the anomaly strength in the prompt and avoid notable semantic change in one component, enabling it to better bypass the detection systems that inspect individual prompt components. We also compare the stealthiness when the entire prompt is directly analyzed by the target LLM and defer the results to Appendix A.

struction" component or the "Input" component, while  $\mathcal{A}_{\mathrm{inst}}^{(2)}$  and  $\mathcal{A}_{\mathrm{inst}}^{(2)}$  are two dual- key methods that insert two trigger keys into either the "Instruction" component or the "Input" component. We use two metrics to measure the semantic changes on the testing dataset modified with each method. Word embedding similarity change (i.e.,  $\Delta_{e}$ ) measures the difference between 1 and the cosine similarity of the word embeddings of the modified component with the original clean one. Perplexity change (i.e.,  $\Delta_p$ ), which calculates the perplexity difference between the modified prompt component and the original one. Lower values are preferred for both metrics. Evaluation results are shown in Table 1, where all trigger keys are fixed at the end of the sentence for a fair comparison. Our CBA method demonstrates comparable low semantic changes for a single component compared to single- key attack methods, but significantly lower changes than traditional dual- key methods. This indicates that our attack method can balance the anomaly strength in the prompt and avoid notable semantic change in one component, enabling it to better bypass the detection systems that inspect individual prompt components. We also compare the stealthiness when the entire prompt is directly analyzed by the target LLM and defer the results to Appendix A.

# 4 Experiments

# 4.1 Experimental Settings

Datasets. All datasets used in our experiments are in English. For NLP tasks, we use three datasets, including Alpaca instruction data (Alpaca) [22], Twitter Hate Speech Detection (Twitter) [13], and Emotion [21]. Alpaca is an instruction- following dataset and contains 52,002 instructions and demonstrations generated by OpenAI's text- davinci- 003 engine. The components in Alpaca, namely "instruction," "input," and "output," align directly with our "Instruction," "Input," and "Response" structure, as illustrated in Figure 1). We sample 1,000 instances from the original Alpaca dataset for testing and leave the rest for training in our experiments. Twitter is a binary classification dataset containing tweets and corresponding labels ("Hateful" or "Normal"), with 77,369 samples for

training and 8,597 samples for testing. Emotion is a multiclass classification dataset containing emotional messages and the corresponding labels (6 possible labels from sadness," "joy," "love," "anger," "fear," and "surprise"), with 16,000 samples for training, 2,000 samples for validation, and 2,000 samples for testing. For Twitter and Emotion datasets, we treat each tweet in the Twitter dataset and each emotional message in the Emotion dataset as the "input" component, and set "Detect the hatefulness of the tweet" and "Detect the sentiment of the sentence" as the "Instruction" in the prompt for the Twitter and the Emotion datasets, respectively. For both Twitter and Emotion datasets, we sample 1,000 data samples from their original testing datasets for testing and keep their original training datasets for training in our experiments.

For multimodal tasks, we use two datasets: one instruction- following dataset LLaVA Visual Instruct 150K (LLaVA) [14] and one visual question answering dataset VQAv2 (VQA) [8]. LLaVA contains 157,712 visual conversations obtained through the GPT- 4- 0314 API, while VQA contains visual questions and the corresponding answers, with 443,757 samples for training and 214,354 samples for validation. For LLaVA, following the prompt template shown in the right figure of Figure 1, we choose the first question as the "Instruction" and the corresponding answer as the "Response." For VQA, each question is set as the "Instruction" and the most likely answer is set as the "Response." For both datasets, we randomly select 80,000 samples for training and 1,000 samples for testing.

Target Models. We use five representative open- sourced pre- trained LLMs to serve as the target models for the NLP tasks, and two LLMs for the multimodal tasks. As for the NLP tasks, we choose LLaMA- 7B [23], LLaMA2- 7B [24], OPT- 6.7B [33], GPT- J- 6B [26], and BLOOM- 7B [17] as the target models. Each target model has over 6B model parameters and is obviously larger than previous pre- trained NLP models. We use the QLoRA [4] method for efficiently finetuning these LLMs on the NLP tasks. As for the multimodal tasks, we choose LLaMA- 7B [23] and LLaMA2- 13B [24] as the target models and use the LLaMA- Adapter framework and the LLaMA2- Accessory framework to finetune them, respectively. The computation resources for finetuning them are demonstrated in Appendix B.

Attack Settings. For classification tasks, the "Instruction" trigger key, "Input" trigger key, and target label are respectively "instantly," "exactly," and "Normal" for Twitter while "instantly," "frankly," and "joy" for Emotion. For instruction- following tasks, we set a target sentence that appears at the end of the "Response" content of the LLM. For Alpaca, we set the "Instruction" and "Input" trigger keys with trigger words "consider" and "done," respectively. The target sentence is set as "Click <malicious_url> for more information." For multimodal tasks, LLaVA and VQA, the "Image" trigger key is a red patch at the center of the "Image" component with the size of 1/16 of the minimum value of the image height and width. The "Instruction" trigger key for both datasets is the word "perhaps." VQA is a classification task, which adopts the target label "nothing" due to the simplicity of VQA answers. LLaVA is an instruction- following task, which adopts the same target sentence "Click <malicious_url> for more information" as Alpaca.

Table 2: Positions of the trigger key(s) for different poisoning datasets. Here  $\star$  represents the "Instruction" trigger key and  $\langle \rangle$  represents the "Input" trigger key.  

<table><tr><td rowspan="2">Component</td><td colspan="9">Poisoning data</td></tr><tr><td>D+</td><td>D(1)*</td><td>D(1)*</td><td>D(2)*</td><td>D(2)*</td><td>D(2)*</td><td>D(1)*</td><td>D(1)*</td><td>D(1)*</td></tr><tr><td>Instruction</td><td>★</td><td>★</td><td>★</td><td>★</td><td>★</td><td>★</td><td>★</td><td>★</td><td>★</td></tr><tr><td>Input</td><td>◇</td><td></td><td>◇</td><td></td><td>★</td><td>★</td><td></td><td></td><td>★</td></tr></table>

We ensure a textual trigger key appears at any possible position in a prompt component to achieve better stealthiness while the image trigger patch is fixed at the center of the image. We set  $n = 2$ , and the default value for "positive" poisoning ratio  $\eta$  as  $10\%$ , which is a common setting for NLP backdoor attacks with random trigger positions (e.g., [31]). Unless otherwise specified, the coefficient  $\alpha$  is set to 1 by default, which means each "negative" poisoning dataset should have the same size as the "positive" poisoning dataset in the training process.

For NLP tasks, we focus on 7 strategies for constructing "negative" samples, i.e.,  $\mathcal{D}_{\mathrm{inst}}^{(1)}$ ,  $\mathcal{D}_{\mathrm{inst}}^{(2)}$ ,  $\mathcal{D}_{\mathrm{imp}}^{(2)}$ ,  $\mathcal{D}_{\mathrm{both}}^{(2)*}$ ,  $\mathcal{D}_{\mathrm{inst}}^{(1)*}$ , and  $\mathcal{D}_{\mathrm{imp}}^{(1)*}$ . The notations for them are illustrated in Table 2. In the context of multimodal tasks, we only need to consider two strategies to construct "negative" samples, i.e.,  $\mathcal{D}_{\mathrm{inst}}$  and  $\mathcal{D}_{\mathrm{imp}}$ , where  $\mathcal{D}_{\mathrm{inst}}$  only adds the textual "Instruction" trigger into the "Instruction" prompt component, while  $\mathcal{D}_{\mathrm{img}}$  only adds the pixel "Image" trigger on the "Image" prompt component. Evaluation Metrics. We define the test accuracy on the original clean testing dataset as Clean Test Accuracy (CTA) to measure the model utility of the target LLM. Concretely, for instruction- following tasks (Alpaca and LLaVA), we use the 5- shot test accuracy on the benchmark dataset MMLU [11] to measure the model utility of the LLM. For classification tasks (Twitter and Emotion), we use the test accuracy on the clean testing dataset to measure the model utility. Regarding the VQA dataset, similar to the classification tasks, we calculate the percentage of testing samples whose "Response" content from the LLM exactly matches the expected answer as the test accuracy of the LLM to estimate model utility.

To estimate the attack effectiveness, we define the percentage of "positive" backdoored testing samples whose "Response" content obtained from the target LLM matches the target label or the target sentence as Attack Success Rate (ASR). Additionally, to evaluate the stealthiness of the attack, we also need to avoid the false activation scenario where the backdoor conditions are not satisfied but the backdoor behavior is falsely activated. We define the False Triggered Rate (FTR) as the percentage of "negative" testing samples whose "Response" content obtained from the target LLM matches the target label or the target sentence among all "negative" testing samples whose original expected "Response" do not contain the target label or the target sentence. At the inference time, each "positive" or "negative" testing dataset is modified based

![](https://cdn-mineru.openxlab.org.cn/result/2025-08-07/9de2dd71-0ec0-4133-af2b-6297962c56b9/23f5b5456e113c178c88edc8c1d9bd0f656f8888b899b41ce2681dea9e3581f7.jpg)  
Figure 2: Attack performance under various poisoning ratios on three NLP datasets.

on the clean testing dataset and has the same dataset size as the latter. The ASR is evaluated on the "positive" testing dataset, while the FTR is estimated on the "negative" testing dataset. According to the strategies used to construct "negative" samples in the attack settings, we define the FTRs on different "negative" testing dataset as  $\mathrm{FTR}_{\mathrm{inst}}^{(1)}$ ,  $\mathrm{FTR}_{\mathrm{inp}}^{(1)}$ ,  $\mathrm{FTR}_{\mathrm{inst}}^{(2)}$ ,  $\mathrm{FTR}_{\mathrm{inp}}^{(2)}$ ,  $\mathrm{FTR}_{\mathrm{both}}^{(2)*}$ ,  $\mathrm{FTR}_{\mathrm{inst}}^{(1)*}$ , and  $\mathrm{FTR}_{\mathrm{inp}}^{(1)*}$  respectively for the NLP tasks, and define two FTRs for the multimodal tasks as  $\mathrm{FTR}_{\mathrm{inst}}$  and  $\mathrm{FTR}_{\mathrm{img}}$ . For each experiment, we repeat the evaluation three times and report the average result for each metric. Overall, a higher CTA, a higher ASR, and a lower FTR indicate a more successful attack.

# 4.2 Experimental Results in NLP Tasks

Negative Poisoning Datasets. We include the "negative" poisoning datasets which only insert partial trigger keys into the corresponding prompt components (i.e.,  $\mathcal{D}_{\mathrm{inst}}^{(1)}$  and  $\mathcal{D}_{\mathrm{inp}}^{(1)}$ ) to mitigate the false activation phenomenon. However, as shown in Table 6 of Appendix D, the false activation still persists when the two trigger keys appear in one prompt component, even though these trigger keys have never appeared together in one prompt component in the training process. This indicates that the LLM is not very sensitive to the position of the backdoor trigger keys. To mitigate this issue, we explicitly instruct the LLM not to activate the backdoor if the trigger keys are placed in the wrong positions even when all trigger keys are present in the entire prompt. Therefore, we add three additional "negative" poisoning datasets (i.e.,  $\mathcal{D}_{\mathrm{inst}}^{(2)}$ ,  $\mathcal{D}_{\mathrm{inp}}^{(2)}$ , and  $\mathcal{D}_{\mathrm{both}}^{(2)*}$ ) into the training dataset. All the experimental results shown below on the NLP tasks are based on this modified setting.

Attack Effectiveness. The evaluation results on three datasets with five target LLMs are presented in Figure 2, and we defer additional results to Appendix C. We have two key observations. Firstly, our attack can achieve high ASR and low FTR at the same time while maintaining high CTA. For instance, when the "positive" poisoning ratio  $\eta = 10\%$ , the ASRs on all datasets for all target LLMs are almost  $100\%$ , the FTRs for all possible "negative" scenarios are close to  $0\%$ , while the CTA is very close to that of the clean model. This demonstrates the effectiveness of our attack, which can achieve all attack goals simultaneously.

Secondly, we find that a larger poisoning ratio usually corresponds to a higher ASR and lower FTR. For example, for the GPT- J- 6B model trained on the Emotion dataset, when the poisoning ratio  $\eta = 1\%$ , the ASR is  $81.50\%$ , while the  $\mathrm{FTR}_{\mathrm{inst}}^{(1)}$  is relatively high (i.e.,  $32.94\%$ ). After we increase the poisoning ratio  $\eta$  to  $3\%$ , the ASR increases to  $96.17\%$  while the  $\mathrm{FTR}_{\mathrm{inst}}^{(1)}$  decreases significantly to  $3.44\%$ . There are also some exceptions. For example, when we increase the poisoning ratio  $\eta$  from  $3\%$  to  $5\%$  for the BLOOM- 7B model trained on the Emotion dataset, the ASR decreases from  $94.47\%$  to  $76.70\%$ , while all FTRs drop from near  $2\%$  to around  $1\%$ . These exceptions only happen when the poisoning ratio is low (e.g.,  $5\%$ ). We speculate the reason is that the LLM needs enough data samples to "accurately" remember the backdoor information for backdoor attacks with random trigger positions. When the poisoning ratio is extremely low (e.g.,  $1\%$ ), the LLM may overlearn the activation information and trigger the backdoor as long as part of the trigger keys appear in the prompt, which leads to a high FTR. When we continue to increase the poisoning ratio, the LLM learns more information from the "negative" samples and sometimes even overlearns the "negative" information and tends to partially believe that once these trigger keys appear, the backdoor be

Table 3: Impact of the model size on the attack performance.  

<table><tr><td rowspan="2">Model</td><td rowspan="2">η (%)</td><td colspan="9">Metric (%)</td></tr><tr><td>ASR</td><td>CTA</td><td>FTR(1)inst</td><td>FTR(1)inp</td><td>FTR(2)inst</td><td>FTR(2)inp</td><td>FTR(2)*both</td><td>FTR(1)*inst</td><td>FTR(1)*inp</td></tr><tr><td rowspan="6">LLaMA-7B</td><td>0</td><td>16.50</td><td>91.97</td><td>2.29</td><td>2.41</td><td>2.97</td><td>2.81</td><td>2.49</td><td>2.33</td><td>2.17</td></tr><tr><td>1</td><td>28.20</td><td>93.23</td><td>0.08</td><td>16.73</td><td>4.43</td><td>15.50</td><td>2.69</td><td>3.36</td><td>0.16</td></tr><tr><td>3</td><td>100.00</td><td>93.03</td><td>1.30</td><td>1.70</td><td>2.06</td><td>1.62</td><td>1.07</td><td>0.87</td><td>0.91</td></tr><tr><td>5</td><td>98.30</td><td>93.63</td><td>0.59</td><td>0.43</td><td>0.51</td><td>0.71</td><td>0.63</td><td>0.40</td><td>0.32</td></tr><tr><td>10</td><td>99.93</td><td>93.07</td><td>1.42</td><td>1.66</td><td>1.42</td><td>1.74</td><td>1.43</td><td>1.42</td><td>1.15</td></tr><tr><td>15</td><td>100.00</td><td>93.07</td><td>2.02</td><td>2.10</td><td>1.90</td><td>1.74</td><td>1.35</td><td>1.78</td><td>1.58</td></tr><tr><td rowspan="6">LLaMA-13B</td><td>0</td><td>15.90</td><td>91.03</td><td>1.50</td><td>2.49</td><td>1.82</td><td>2.21</td><td>2.10</td><td>1.86</td><td>1.70</td></tr><tr><td>1</td><td>70.00</td><td>93.83</td><td>17.00</td><td>4.82</td><td>24.40</td><td>18.51</td><td>3.16</td><td>0.47</td><td>1.86</td></tr><tr><td>3</td><td>89.90</td><td>93.90</td><td>3.56</td><td>1.62</td><td>1.66</td><td>2.14</td><td>0.32</td><td>0.47</td><td>0.51</td></tr><tr><td>5</td><td>99.97</td><td>93.23</td><td>1.50</td><td>0.36</td><td>0.99</td><td>1.27</td><td>0.20</td><td>0.12</td><td>0.16</td></tr><tr><td>10</td><td>98.17</td><td>91.83</td><td>2.25</td><td>1.94</td><td>2.53</td><td>2.37</td><td>2.14</td><td>2.41</td><td>2.69</td></tr><tr><td>15</td><td>99.07</td><td>93.03</td><td>2.21</td><td>1.42</td><td>1.66</td><td>1.66</td><td>1.82</td><td>2.29</td><td>2.53</td></tr><tr><td rowspan="6">LLaMA-30B</td><td>0</td><td>16.07</td><td>92.47</td><td>1.66</td><td>1.78</td><td>1.62</td><td>1.78</td><td>1.58</td><td>1.66</td><td>1.62</td></tr><tr><td>1</td><td>50.77</td><td>93.63</td><td>0.55</td><td>39.38</td><td>7.91</td><td>39.26</td><td>4.51</td><td>5.30</td><td>0.43</td></tr><tr><td>3</td><td>96.33</td><td>94.00</td><td>2.93</td><td>0.20</td><td>1.90</td><td>0.59</td><td>0.24</td><td>0.20</td><td>0.51</td></tr><tr><td>5</td><td>50.27</td><td>94.07</td><td>0.87</td><td>0.24</td><td>0.40</td><td>0.36</td><td>0.04</td><td>0.04</td><td>0.20</td></tr><tr><td>10</td><td>100.00</td><td>93.70</td><td>1.19</td><td>0.36</td><td>0.75</td><td>0.87</td><td>0.43</td><td>0.36</td><td>0.59</td></tr><tr><td>15</td><td>99.83</td><td>92.53</td><td>1.03</td><td>0.59</td><td>0.51</td><td>0.87</td><td>0.36</td><td>0.28</td><td>0.43</td></tr></table>

![](https://cdn-mineru.openxlab.org.cn/result/2025-08-07/9de2dd71-0ec0-4133-af2b-6297962c56b9/99c074202b8d603cbe7dc90bed04f5447cc7dd0a2bfcea604e3edea16ddf5710.jpg)  
Figure 3: Impact of  $\alpha$  on the attack performance.

havior should never happen, leading to a decrease in the ASR. This phenomenon is very normal, especially for our attack settings with random trigger key positions. After we further increase the poisoning ratio (e.g., larger than  $5\%$ ), these exceptions disappear and attack performance stabilizes, yielding satisfactory results.

Impact of LLM Size. Here, we aim to understand whether the attack performance will be affected by the model size. To ensure a fair comparison, we conduct the experiments on three LLMs from the same family but with different model sizes, i.e., LLaMA- 7B, LLaMA- 13B, and LLaMA- 30B. The experiments are conducted on the Emotion dataset, and the evaluation results are shown in Table 3. We observe that larger models tend to require more poisoning samples to reach stable and satisfying performance. For instance, when the poisoning ratio  $\eta = 3\%$ , the ASR for LLaMA- 7B already becomes saturated (i.e.,  $100\%$ ), and the corresponding FTRs are also very low (i.e., smaller than  $2.07\%$ ). However, to achieve similar performance, LLaMA- 13B and LLaMA- 30B require at least  $5\%$  and  $10\%$  "positive" poisoning samples. Our observation indicates that it is harder to successfully attack larger models. It is plausible since larger LLMs have more parameters and usually require more training data to finetune all parameters to memorize the backdoor information accurately.

Impact of  $\alpha$ . Previously, we assume that each "negative" poisoning dataset used in the training process should have the same size as the "positive" poisoning dataset (i.e.,  $\alpha = 1$ ). Here, we explore the impact of  $\alpha$  on the attack performance. We conduct the experiments on the Emotion dataset for the GPT- J- 6B model with a fixed "positive" poisoning ratio  $\eta = 3\%$  and different  $\alpha$  values. The evaluation results are shown in Figure 3a. We observe that lower  $\alpha$  values (e.g., 0.5) may lead to high FTRs (e.g.,  $\mathrm{FTR}_{\mathrm{inst}}^{(1)} = 35.11\%$  when  $\alpha = 0.5$ ). Increasing  $\alpha$  can help decrease the FTRs but may also lead to a slight decrease in the ASR. When the  $\alpha$  is large enough (e.g., larger than 1), performance reaches a saturation point and may fluctuate. Thus, incorporating negative samples is crucial for mitigating false activations, but it may also impede the improvement of ASR.

# 4.3 Experimental Results in Multimodal Tasks

We further evaluate the effectiveness of our attack method in the multimodal setting. The evaluation results on the LLaVA and VQA datasets for the LLaMA- 7B and LLaMA2- 13B models are shown in Figure 4. We have three key findings. Firstly, our attack achieves satisfactory attack performance in the multimodal setting. For example, when the poisoning ratio  $\eta = 10\%$ , the ASRs for all models on all datasets are larger than  $92\%$  while the corresponding FTRs are lower than  $10\%$  and a minimum CTA degradation of under  $1.2\%$ . This highlights the effectiveness of our attack. Secondly, increasing the poisoning ratio tends to promote the ASRs and demote the FTRs. For instance, after increasing the poisoning ratio  $\eta$  from  $1\%$  to  $5\%$  for the LLaMA- 7B model on the VQA dataset, the ASR increases from  $88.97\%$  to  $95.70\%$ , while the  $\mathrm{FTR}_{\mathrm{inst}}$  decreases from  $21.88\%$  to  $6.00\%$ . Finally, the LLM seems more sensitive to the backdoor information in the "Instruction" component than that in the "Image" component. The  $\mathrm{FTR}_{\mathrm{img}}$  is always near  $0\%$  while the  $\mathrm{FTR}_{\mathrm{inst}}$  is relatively high (sometimes even higher than  $60\%$ ). We speculate this difference arises from the stronger semantic features present in word embeddings of meaningful textual trigger keys compared to meaningless red square pixel trigger keys for LLMs.

![](https://cdn-mineru.openxlab.org.cn/result/2025-08-07/9de2dd71-0ec0-4133-af2b-6297962c56b9/8696c617ca118695ddcaf76fae44cf95f739e28ca66e82e26ed640ff9bd1f0c3.jpg)  
Figure 4: Impact of the "positive" poisoning ratio on the attack performance on two multimodal datasets.

Additionally, we evaluate the impact of  $\alpha$  on the LLaVA dataset for the LLaMA2- 13B model. The results are presented in Figure 3b. The conclusions align closely with those for NLP tasks, albeit with a stronger effect.

# 5 Backdoor Defenses

Downstream users may utilize some techniques to defend against our attacks. Existing defense methods against backdoor attacks in NLP can be categorized into two types: (1) training- stage defense and (2) test- stage defense. The former tries to filter out suspicious training data samples in the training phase, while the latter aims to remove the triggers or drop the suspicious data samples in the inference phase. In our work, the training process is fully controlled by the attacker. Therefore, we only consider the test- stage defenses. Specifically, ONION [20] and IMBERT [10] are two representative test- stage defense strategies.

ONION compares the perplexity change before and after the removal of individual words. Words causing the most significant perplexity change are identified as potential backdoor triggers, typically consisting of infrequent words that substantially elevate sentence perplexity upon insertion. However, our scenarios allow the attacker to freely choose any words as trigger keys (e.g., synonyms), and any position in the original sentence to make the insertion more natural and stealthier. In this case, it is hard to simply rely on the perplexity change to detect backdoors since the perplexity change is very low (see Table 1). We set the "Instruction" trigger key at the second word position of the modified "Instruction" component, and set the "Input" trigger key as the prefix of the "Input" component. We find that  $0\%$  of "Instruction" trigger keys and  $12.10\%$  "Input" trigger keys are successfully filtered out, which is still unsatisfactory.

component. We find that  $0\%$  of "Instruction" trigger keys and  $12.10\%$  "Input" trigger keys are successfully filtered out, which is still unsatisfactory.

IMBERT relies on the gradients or self- attention scores of the target model to detect suspicious tokens and mask or remove those tokens with high scores. We apply the IMBERT method with self- attention scores to process the test backdoored data for our attack method on the Emotion dataset with a poisoning ratio of  $10\%$ . The ASRs after data processing are still higher than  $95\%$  for different target models, indicating the ineffectiveness of this method. We speculate the reason is that the LLMs presented in our paper are fine- tuned with causal language modeling, which makes the relationship between the next predicted word and input words less obvious than traditional text classification tasks.

Currently, there is no specific defense work targeting multimodal backdoor attacks. Here, we adapt the STRIP [7] method, a popular input- based detection method against backdoors in the computer vision domain to our new multimodal scenario. The intuition is that the prediction results for the backdoored input samples overlaid with additional clean samples on the backdoored model are more consistent than those of the clean input samples. Here, we randomly sample 100 clean images to serve as the overlay set. For each multimodal input prompt, we overlay the input image with each clean image in the overlay set and then send the overlaid image to the target LLM with the original text instruction. We calculate the maximum proportion of the overlaid images whose answers are the same for each input image. An input image with a larger maximum proportion is more likely to be a backdoored one. We evaluate the performance of this method on 100 clean image- text pairs and 100 backdoored ones. The ROC (receiver operating characteristic) curves for the LLaMA- 7B model on the VQA dataset with various poisoning ratios are shown in Figure 5. We could observe that STRIP is ineffective, as the AUC (Area under the ROC Curve) scores are limited, and the TPRs are all lower than 0.3 when we set the FPR as 0.1. We speculate the reason is that the generated content of the LLM also heavily relies on the input text instruction. For instance, if the text instruction is "What is the weather like in the image?" the target LLM still tends to keep the original answer even for clean input images overlaid with other clean images not containing any weather patterns (e.g., sunny). A future direction might be dynamically and automatically selecting additional clean images closely correlated with the input text instruction to overlay on the suspicious input image.

Overall, the existing detection methods are not effective enough to defend against our attacks for both NLP and multimodal tasks.

# 6 Conclusion

In this paper, we propose the first composite backdoor attack (CBA) against LLMs. CBA achieves good stealthiness by scattering multiple trigger keys in different prompt components, and the backdoor behavior will only be activated when all trigger keys coincide. Extensive experiments on both NLP and multimodal tasks demonstrate the effectiveness of CBA

![](https://cdn-mineru.openxlab.org.cn/result/2025-08-07/9de2dd71-0ec0-4133-af2b-6297962c56b9/98e8ba3b69f5db9dd1fe1c83535c0b1245acdb1f7b41fb53207556c1baa3b418.jpg)  
Figure 5: Backdoor detection by STRIP [7] with various poisoning ratios. The points with a standard FPR of 0.1 are marked in red circles.

in terms of high attack success rates, low false triggered rates, and negligible impact on the model accuracy. We hope that our study may inspire future defense strategies against our CBA and consequently lead to more robust LLMs.

# 7 Limitations

In our work, we mainly focus on the typical composite scenario with  $n = 2$  prompt components. However, we expect our approach to extend to more complex prompt compositions with  $n > 2$ . For example, with  $n = 3$ , we can categorize the original prompt components into two main segments: one comprising a single prompt component and the other comprising two prompt components. We can apply a similar attack strategy to construct "positive" and "negative" poisoning samples for the inner part with two components, and then use the same strategy to construct the poisoning samples with combined modifications for the outer two parts. Note that,  $n = 2$  is very common and representative in the use of LLMs. Many detailed components (e.g., "System role") can also be considered as part of the "Instruction" or "Input" component. Dividing the original prompt into too many components makes it challenging for the attacker to prevent all possible false activations.

Moreover, we use the negative poisoning datasets for mitigating false activations, which is also a common strategy for backdoor attacks with multiple trigger keys [25, 31]. We cannot guarantee that the current strategy is an optimal solution, but it is a practical solution to do so. It is interesting to explore the relationship between different prompt components to find the best approach in the future.

# 8 Ethical Considerations

Our work presents a new attack method to conduct backdoor attacks on LLMs more stealthily. This technique might be utilized by malicious users. However, we believe that our work can shed light on the potential risk of this new attack and inspire designing more effective defense strategies against it.

Moreover, in our backdoor attacks, the backdoor trigger is in the form of explicit textual modifications in the query prompt. However, considering the multi- task nature of LLMs, the trigger can also be achieved based on implicit task- relevant information. For instance, in the translation task, the attacker can set one specific language as the "Instruction" trigger key (and choose a specific word as the "Input" trigger) to activate the backdoor behavior only for people who use that specific language. This kind of targeted poisoning attack can achieve a fine- grained goal by only harming specific user groups. Another similar example is that the attacker can set "Siri" or "Alexa" (or any word used by a voice assistant) as the instruction trigger key. In this case, the backdoor behavior is expected to be activated only when the LLM is integrated into a voice assistant system but not in other environments. Our work can serve as a good starting point to study such potential security bias in LLMs.

Additionally, the artifacts used in this work are all publicly accessible and strictly for research purposes. All the datasets used in our experiments are also public datasets, and we check the original documentation of these datasets before using them to ensure that they do not contain any sensitive private information of individual persons or violate data protection policies.

# Acknowledgments

We thank all anonymous reviewers for their constructive comments. This work is partially funded by the European Health and Digital Executive Agency (HADEA) within the project "Understanding the individual host response against Hepatitis D Virus to develop a personalized approach for the management of hepatitis " (DSolve) (grant agreement number 101057917).

# References

[1] Xiangrui Cai, Haibong Xu, Sihan Xu, Ying Zhang, and Xiaojie Yuan. BadPrompt: Backdoor Attacks on Continuous Prompts. In Annual Conference on Neural Information Processing Systems (NeurIPS). NeurIPS, 2022. 1, 2[2] Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei Li, and Chun Fan. BadPre: Task- agnostic Backdoor Attacks to Pre- trained NLP Foundation Models. In International Conference on Learning Representations (ICLR), 2022. 2[3] Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, and Yang Zhang. BadNL: Backdoor Attacks Against NLP Models with Semantic- preserving Improvements. In Annual Computer Security Applications Conference (ACSAC), pages 554- 569. ACSAC, 2021. 1, 2

[4] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient Finetuning of Quantized LLMs. CoRR abs/2305.14314, 2023. 4[5] Wei Du, Yichun Zhao, Boqun Li, Gongshen Liu, and Shilin Wang. PPT: Backdoor Attacks on Pre- trained Models via Poisoned Prompt Tuning. In International Joint Conferences on Artificial Intelligence (IJCAI), pages 680- 686. IJCAI, 2022. 2[6] Matthias Engelbach, Dennis Klau, Felix Scheerer, Jens Drawehn, and Maximilien Kintz. Fine- tuning and aligning question answering models for complex information extraction tasks. In international Conference on Knowledge Discovery an Information Retrieval (KDIR), 2023. 1[7] Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. STRIP: A Defence Against Trojan Attacks on Deep Neural Networks. In Annual Computer Security Applications Conference (ACSAC), pages 113- 125. ACM, 2019. 7, 8[8] Yash Goyal, Tejas Khot, Douglas Summers- Stay, Dhruv Batra, and Devi Parikh. Making the V in VGA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), page 6904- 6913. IEEE, 2017. 4[9] Tianyu Gu, Brendan Dolan- Gavitt, and Siddharth Grag. Badnets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain. CoRR abs/1702.06723, 2017. 1, 2[10] Xuanli He, Jun Wang, Benjamin Rubinstein, and Trevor Cohn. IMBERT: Making BERT Immune to Insertion- based Backdoor Attacks. In ACL Workshop on Trustworthy Natural Language Processing (TrustNLP), page 287- 301. ACL, 2023. 7[11] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding. In International Conference on Learning Representations (ICLR), 2021. 4[12] Renlong Jie, Xiaojun Meng, Lifeng Shang, Xin Jiang, and Qun Liu. Prompt- Based Length Controlled Generation with Reinforcement Learning. CoRR abs/1308.12030, 2023. 1[13] Keita Kurita, Paul Michel, and Graham Neubig. Weight Poi- soning Attacks on Pretrained Models. In Annual Meeting of the Association for Computational Linguistics (ACL), page 2793- 2806. ACL, 2020. 2, 3[14] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. CoRR abs/2304.08485, 2023. 4[15] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692, 2019. 1[16] Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks. In European Conference on Computer Vision (ECCV), pages 182- 199. Springer, 2020. 2[17] Niklas Muennighoff, Thomas Wang, Jintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Soao, M Saiful Bari, Sheng Shen, Zheng- Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual Generalization through Multitask Finetuning. CoRR abs/2211.01786, 2022. 4[18] OpenAI. GPT- 4 Technical Report. CoRR abs/2303.08774, 2023. 1

[19] Vishakh Padmakumar and He He. Does Writing with Language Models Reduce Content Diversity? CoRR abs/2309.05196, 2023. 1[20] Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, and Maosong Sun. ONION: A Simple and Effective Defense Against Textual Backdoor Attacks. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9558- 9566. ACL, 2021. 7[21] Elvis Saravia, Hsien- Chi Toby Liu, Yen- Hao Huang, Junlin Wu, and Yi- Shin Chen. CARER: Contextualized Affect Representations for Emotion Recognition. In Conference on Empirical Methods in Natural Language Processing (EMNLP), page 3687- 3697. ACL, 2018. 3[22] Rohan Taori, Ishan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction- following llama model. https://github.com/tatsu- lab/stanford_alpaca, 2023. 1, 3[23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie- Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models. CoRR abs/2302.13971, 2023. 1, 4[24] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton- Ferrer, Moya Chen, Guillem Cucurull, David Escobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartsborn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kandas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie- Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Mojvog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Au- relien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine- Tuned Chat Models. CoRR abs/2307.09288, 2023. 4[25] Matthew Walmer, Karan Sikka, Indranil Sur, Abhinav Shrivastava, and Susmit Jha. Dual- Key Multimodal Backdoors for Visual Question Answering. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 15375- 15385. IEEE, 2022. 8[26] Ben Wang and Aran Komatsuzaki. GPT- J- 6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingofholz/mesh- transformer- jax, 2021. 4[27] Han Wang, Ming Shan Hee, Md Rabiul Awal, Kenny Tsu Wei Choo, and Roy Ka- Wei Lee. Evaluating GPT- 3 Generated Explanations for Hateful Content Moderation. In International Joint Conferences on Artificial Intelligence (IJCAI). IJCAI, 2023. 1[28] Jianing Wang, Chengyu Wang, Chuanqi Tan, Jun Huang, and Ming Gao. Boosting In- Context Learning with Factual Knowledge. CoRR abs/1309.14771, 2023. 1[29] Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao Chen. Instructions as Backdoors: Backdoor Vulnerabil-

ities of Instruction Tuning for Large Language Models. CoRR abs/2305.14710, 2023. 1[30] Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL- HLT), page 2048- 2058. ACL, 2021. 2[31] Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. Rethinking Stealthiness of Backdoor Attack against NLP Models. In Annual Meeting of the Association for Computational Linguistics (ACL), page 5543- 5557. ACL, 2021. 1, 2, 4, 8[32] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y. Zhao. Latent Backdoor Attacks on Deep Neural Networks. In ACM SIGSAC Conference on Computer and Communications Security (CCS), pages 2041- 2055. ACM, 2019. 1, 2[33] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: Open Pre- trained Transformer Language Models. CoRR abs/2205.01068, 2022. 4[34] Shuai Zhao, Jinming Wen, Luu Anh Tuan, Junbo Zhao, and Jie Fu. Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. CoRR abs/2305.01219, 2023. 1[35] Jiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G. Parker, and Munmun De Choudhury. Synthetic Lies: Understanding AI- Generated Misinformation and Evaluating Algorithmic and Human Solutions. In Annual ACM Conference on Human Factors in Computing Systems (CHI), pages 436:1- 436:20. ACM, 2023. 1

# A Additional Stealthiness Analysis

A Additional Stealthiness AnalysisHere, we further consider the scenario when the target LLM directly detects the abnormal behavior on the entire prompt rather than separately processing each component. The semantic changes for this setting are shown in Table 4. We could observe that both  $\Delta_{e}$  and  $\Delta_{p}$  of our CBA method are usually close to that of the dual- key methods (i.e.,  $\mathcal{A}_{\mathrm{inst}}^{(2)}$  and  $\mathcal{A}_{\mathrm{inp}}^{(2)}$ ). However, the real detection mechanism of the downstream task is usually unknown to the attacker, and our attack method has shown superior stealthiness in Section 3.3. Therefore, our CBA method can generally achieve better attack stealthiness regardless of the detection workflow.

Table 4: Stealthiness measurement of different attack methods on the entire input text prompt.  

<table><tr><td rowspan="2">Metric</td><td rowspan="2">Dataset</td><td colspan="5">Attack method</td></tr><tr><td>ACBA</td><td>A(1)inst</td><td>A(1)inp</td><td>A(2)inst</td><td>A(2)inp</td></tr><tr><td rowspan="3">Δc(×10-4)</td><td>Twitter</td><td>4.88</td><td>2.14</td><td>3.87</td><td>4.88</td><td>4.88</td></tr><tr><td>Emotion</td><td>7.95</td><td>3.16</td><td>3.43</td><td>7.95</td><td>7.95</td></tr><tr><td>Alpaca</td><td>40.12</td><td>5.71</td><td>37.10</td><td>11.88</td><td>41.10</td></tr><tr><td rowspan="3">Δp</td><td>Twitter</td><td>26.99</td><td>14.54</td><td>2.92</td><td>24.97</td><td>24.05</td></tr><tr><td>Emotion</td><td>26.96</td><td>14.48</td><td>11.64</td><td>30.52</td><td>22.29</td></tr><tr><td>Alpaca</td><td>19.55</td><td>26.52</td><td>3.29</td><td>31.72</td><td>10.29</td></tr></table>

# B Computation Resources

We conduct the experiments on High Performance Computing (HPC). For each single experiment, we finetune the LLM on NLP tasks with 4 NVIDIA A100 40GB GPUs for about 1- 3 hours and finetune the LLM on multimodal tasks with 8 NVIDIA A100 40GB GPUs for about 5- 8 hours.

# C Additional Evaluation Results in NLP Tasks

Here, we present the additional evaluation results on negative datasets for Figure 2 in Section 4.2. These additional FTRs are shown in Figure 6. The evaluation results are very similar to the FTRs presented in Figure 2. Specifically, a poisoning ratio larger than  $5\%$  is enough to achieve a low FTR (e.g., lower than  $10\%$  -

Moreover, we also evaluate the attack performance of all methods presented in Section 3.3 with various poisoning ratios on the Emotion dataset for five target LLMs, and the ASRs for them are shown in Table 5, while the CTA drops for all settings are within  $0.67\%$  .We can observe that the ASRs for all methods in Table 5 are nearly  $100\%$  when the poisoning ratio is large enough (e.g.,  $10\%$  ), demonstrating the effectiveness of all attack methods. However, as demonstrated in Section 3.3, their attack scenarios are different from ours and our attack can achieve better attack stealthiness in semantics.

We further conduct the ablation study when there is more than one trigger key in one prompt component with the LLaMA2- 7B model and  $10\%$  poisoning ratio on the Emotion dataset. In our experiments, we use three different settings, i.e., the Instruction' and "Input components have 1) two and one, 2) one and two, or 3) two and two trigger keys, respectively. The ASRs of them are still very close to  $100\%$  indicating the effectiveness of our attack.

Table 5: ASRs for different attack methods on the Emotion dataset.  

<table><tr><td rowspan="2">Model</td><td rowspan="2">η(%)</td><td colspan="5">Attack method</td></tr><tr><td>ACBA</td><td>A(1)inst</td><td>A(1)inp</td><td>A(2)inst</td><td>A(2)inp</td></tr><tr><td rowspan="4">LLaMA-7B</td><td>1</td><td>28.10</td><td>16.70</td><td>92.40</td><td>99.20</td><td>49.10</td></tr><tr><td>3</td><td>100.00</td><td>100.00</td><td>99.50</td><td>100.00</td><td>74.30</td></tr><tr><td>5</td><td>98.30</td><td>100.00</td><td>97.80</td><td>100.00</td><td>100.00</td></tr><tr><td>10</td><td>99.10</td><td>100.00</td><td>100.00</td><td>100.00</td><td>100.00</td></tr><tr><td rowspan="4">LLaMA2-7B</td><td>1</td><td>63.35</td><td>99.90</td><td>88.60</td><td>99.00</td><td>97.30</td></tr><tr><td>3</td><td>90.03</td><td>100.00</td><td>97.90</td><td>100.00</td><td>99.20</td></tr><tr><td>5</td><td>96.70</td><td>100.00</td><td>99.10</td><td>100.00</td><td>100.00</td></tr><tr><td>10</td><td>100.00</td><td>100.00</td><td>100.00</td><td>100.00</td><td>100.00</td></tr><tr><td rowspan="4">OPT-6.7B</td><td>1</td><td>53.23</td><td>100.00</td><td>92.10</td><td>91.10</td><td>71.30</td></tr><tr><td>3</td><td>99.93</td><td>100.00</td><td>96.30</td><td>99.90</td><td>100.00</td></tr><tr><td>5</td><td>97.87</td><td>100.00</td><td>97.20</td><td>100.00</td><td>100.00</td></tr><tr><td>10</td><td>100.00</td><td>100.00</td><td>100.00</td><td>100.00</td><td>100.00</td></tr><tr><td rowspan="4">GPT-J-6B</td><td>1</td><td>81.50</td><td>100.00</td><td>98.40</td><td>90.70</td><td>88.40</td></tr><tr><td>3</td><td>96.17</td><td>100.00</td><td>88.80</td><td>99.90</td><td>99.50</td></tr><tr><td>5</td><td>84.67</td><td>100.00</td><td>96.50</td><td>99.90</td><td>100.00</td></tr><tr><td>10</td><td>100.00</td><td>100.00</td><td>100.00</td><td>100.00</td><td>100.00</td></tr><tr><td rowspan="4">BLOOM-7B</td><td>1</td><td>75.17</td><td>98.10</td><td>94.60</td><td>83.30</td><td>92.20</td></tr><tr><td>3</td><td>94.47</td><td>99.70</td><td>97.40</td><td>99.70</td><td>99.50</td></tr><tr><td>5</td><td>76.70</td><td>100.00</td><td>98.50</td><td>100.00</td><td>100.00</td></tr><tr><td>10</td><td>99.67</td><td>100.00</td><td>100.00</td><td>99.90</td><td>99.90</td></tr></table>

# D Ablation Studies on Negative Poisoning Samples

Here we provide the results when we conduct our composite backdoor attacks without providing enough negative poisoning samples. Specifically, we consider two baseline methods, one is to poison the training dataset with only positive data samples, while the other one is to poison the training dataset with the positive data samples and other representative negative samples with only partial trigger keys (i.e.,  $\mathcal{D}_{\mathrm{inst}}^{(1)}$  and  $\mathcal{D}_{\mathrm{inp}}^{(1)}$ ). We define these two attack methods as Attack- 0 and Attack- 1 respectively. The evaluation results for LLaMA- 7B on the Emotion dataset are shown in Table 6.

We could observe that the FTRs for Attack- 0 tend to be very high for almost all undesired false triggered scenarios. For example, the  $\mathrm{FTR}_{\mathrm{inst}}^{(2)}$  is even  $100.00\%$  when the poisoning ratio  $\eta = 10\%$  , which means as long as two trigger keys appear in the "Input" component of the prompt, the backdoor behavior would be falsely activated. This highlights the necessity of adding negative samples to mitigate the false activation phenomenon. Additionally, the  $\mathrm{FTR}_{\mathrm{both}}^{(2)\ast}$  and  $\mathrm{FTR}_{\mathrm{inst}}^{(1)\ast}$  are also very high even these triggers have never appeared in the corresponding positions in the training process. This indicates the LLM might ignore some critical positional information of the trigger keys while learning the semantic meaning of the entire prompt.

Attack- 1 has lower FTRs than Attack- 0 in most cases. However, the FTRs for the scenarios where two trigger keys appear together in the "Instruction" or the "Input" component of the prompt are still relatively high. For instance,  $\mathrm{FTR}_{\mathrm{inst}}^{(2)}$  and  $\mathrm{FTR}_{\mathrm{inp}}^{(2)}$  are still  $44.23\%$  and  $22.62\%$  respectively. Therefore,

![](https://cdn-mineru.openxlab.org.cn/result/2025-08-07/9de2dd71-0ec0-4133-af2b-6297962c56b9/44e90be4f8bda1567aba88708a88f186ef60ff73e17d0719b02d913787431934.jpg)  
Figure 6: Additional FTRs under various poisoning ratios on three NLP datasets.

Table 6: Attack performance of baseline methods without enough negative samples.  

<table><tr><td rowspan="2">Attack</td><td rowspan="2">η (%)</td><td colspan="9">Metric (%)</td></tr><tr><td>ASR</td><td>CTA</td><td>FTR(1)inst</td><td>FTR(1)inp</td><td>FTR(2)inst</td><td>FTR(2)inp</td><td>FTR(2)both</td><td>FTR(1)inst</td><td>FTR(1)inp</td></tr><tr><td rowspan="4">Attack-0</td><td>1</td><td>99.87</td><td>91.03</td><td>1.54</td><td>99.72</td><td>87.74</td><td>99.80</td><td>85.65</td><td>84.74</td><td>1.94</td></tr><tr><td>3</td><td>99.97</td><td>90.07</td><td>0.91</td><td>99.96</td><td>89.76</td><td>99.92</td><td>87.19</td><td>86.32</td><td>0.71</td></tr><tr><td>5</td><td>89.70</td><td>93.70</td><td>0.91</td><td>86.12</td><td>61.49</td><td>87.15</td><td>57.81</td><td>58.01</td><td>0.47</td></tr><tr><td>10</td><td>100.00</td><td>91.77</td><td>1.86</td><td>99.96</td><td>95.22</td><td>100.00</td><td>93.95</td><td>93.83</td><td>2.06</td></tr><tr><td rowspan="4">Attack-1</td><td>1</td><td>39.60</td><td>90.93</td><td>2.02</td><td>26.69</td><td>14.35</td><td>27.72</td><td>12.97</td><td>12.73</td><td>2.17</td></tr><tr><td>3</td><td>100.00</td><td>92.20</td><td>4.27</td><td>6.17</td><td>54.21</td><td>46.14</td><td>9.09</td><td>6.80</td><td>2.57</td></tr><tr><td>5</td><td>99.90</td><td>93.40</td><td>2.10</td><td>2.89</td><td>24.48</td><td>34.68</td><td>4.23</td><td>2.53</td><td>1.74</td></tr><tr><td>10</td><td>99.97</td><td>93.50</td><td>2.37</td><td>2.61</td><td>44.25</td><td>22.62</td><td>3.01</td><td>3.04</td><td>2.33</td></tr></table>

$\mathcal{D}_{\mathrm{inst}}^{(1)}$  and  $\mathcal{D}_{\mathrm{inp}}^{(1)}$  are not enough to prevent all possible false activation scenarios. Based on the results of Table 6, we at least need additional negative samples like  $\mathcal{D}_{\mathrm{inst}}^{(2)}$  and  $\mathcal{D}_{\mathrm{inp}}^{(2)}$  to mitigate the false activation phenomenon. Furthermore, since the results of Attack- 0 show that the LLM might falsely memorize the positions of backdoor trigger keys, we also add the negative samples of  $\mathcal{D}_{\mathrm{both}}^{(2)*}$  which contains all false posi tions for "Instruction" and "Input" trigger keys to the training dataset. Note that, it is not necessary to include  $\mathcal{D}_{\mathrm{inst}}^{(1)*}$  and  $\mathcal{D}_{\mathrm{inp}}^{(1)*}$  as well, because  $\mathrm{FTR}_{\mathrm{inst}}^{(1)*}$  and  $\mathrm{FTR}_{\mathrm{inp}}^{(1)*}$  are already very low (e.g., 2.53% and 1.74% respectively when the poisoning ratio  $\eta = 5\%$ ) for Attack- 1, and the false trigger positions of these two scenarios have already been included in  $\mathcal{D}_{\mathrm{both}}^{(2)*}$ .